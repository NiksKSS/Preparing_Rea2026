{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fa9b44a",
   "metadata": {},
   "source": [
    "\n",
    "# üîπ –£–°–¢–ê–ù–û–í–ö–ê + –í–´–ë–û–† –ú–û–î–ï–õ–ò\n",
    "\n",
    "---\n",
    "\n",
    "##  –£—Å—Ç–∞–Ω–æ–≤–∫–∞ (–¢–ï–†–ú–ò–ù–ê–õ)\n",
    "\n",
    "```bash\n",
    "sudo apt update\n",
    "sudo apt upgrade -y\n",
    "sudo apt install -y curl git python3 python3-pip python3-venv\n",
    "```\n",
    "\n",
    "\n",
    "##  –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞\n",
    "\n",
    "```bash\n",
    "mkdir ml_championship\n",
    "cd ml_championship\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "##  –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ Python\n",
    "\n",
    "```bash\n",
    "python3 -m venv venv\n",
    "source venv/bin/activate\n",
    "```\n",
    "\n",
    "\n",
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Jupyter Notebook\n",
    "\n",
    "```bash\n",
    "pip install notebook jupyterlab\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "–ü—Ä–æ–≤–µ—Ä–∫–∞:\n",
    "\n",
    "```bash\n",
    "ollama --version\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "##  –ó–∞–ø—É—Å–∫ Ollama\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "‚ùó –¢–µ—Ä–º–∏–Ω–∞–ª –Ω–µ –∑–∞–∫—Ä—ã–≤–∞—Ç—å\n",
    "\n",
    "\n",
    "## –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "\n",
    "```bash\n",
    "ollama pull mistral:7b\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "##  –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ \n",
    "\n",
    "| –ú–æ–¥–µ–ª—å         | –†–∞–∑–º–µ—Ä  | RAM     | –î–ª—è —á–µ–≥–æ –ª—É—á—à–µ –≤—Å–µ–≥–æ |\n",
    "| -------------- | ------- | ------- | -------------------- |\n",
    "| **mistral:7b** | ~4.1 GB | 8‚Äì10 GB | —É–Ω–∏–≤–µ—Ä—Å–∞–ª, RAG       |\n",
    "| llama2:7b      | ~3.8 GB | 8 GB    | –ø—Ä–æ—Å—Ç–æ–π —á–∞—Ç          |\n",
    "| llama2:13b     | ~7.3 GB | 16 GB   | —Å–ª–æ–∂–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã       |\n",
    "| gemma:2b       | ~1.5 GB | 4 GB    | —Å–ª–∞–±—ã–µ –ü–ö            |\n",
    "| gemma:7b       | ~4 GB   | 8 GB    | —Ç–µ–∫—Å—Ç—ã, –∞–Ω–∞–ª–∏–∑       |\n",
    "| codellama:7b   | ~4 GB   | 8 GB    | –∫–æ–¥                  |\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ –î–û–û–ë–£–ß–ï–ù–ò–ï (FINE-TUNING)\n",
    "\n",
    "---\n",
    "\n",
    "##  –ö–∞–∫ Ollama –¥–µ–ª–∞–µ—Ç fine-tuning\n",
    "\n",
    "Ollama –ø—Ä–∏–Ω–∏–º–∞–µ—Ç:\n",
    "\n",
    "```\n",
    "ollama train <–±–∞–∑–æ–≤–∞—è_–º–æ–¥–µ–ª—å> <—Ñ–∞–π–ª_—Å_–¥–∞–Ω–Ω—ã–º–∏> <–Ω–æ–≤–æ–µ_–∏–º—è_–º–æ–¥–µ–ª–∏>\n",
    "```\n",
    "\n",
    "–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –≤–Ω—É—Ç—Ä–∏:\n",
    "\n",
    "1. –¥–∞–Ω–Ω—ã–µ —á–∏—Ç–∞—é—Ç—Å—è –ø–æ—Å—Ç—Ä–æ—á–Ω–æ\n",
    "2. –º–æ–¥–µ–ª—å –¥–æ–æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ –Ω–∏—Ö\n",
    "3. —Å–æ–∑–¥–∞—ë—Ç—Å—è **–Ω–æ–≤–∞—è –º–æ–¥–µ–ª—å**, —Å—Ç–∞—Ä–∞—è –Ω–µ –ø–æ—Ä—Ç–∏—Ç—Å—è\n",
    "\n",
    "\n",
    "##  –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "\n",
    "\n",
    "##  –ó–∞–ø—É—Å–∫ fine-tuning (–¢–ï–†–ú–ò–ù–ê–õ)\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama train mistral:7b data/text.txt mistral_text_ft\n",
    "```\n",
    "\n",
    "```bash\n",
    "ollama train mistral:7b data/table.csv mistral_table_ft\n",
    "```\n",
    "\n",
    "\n",
    "##  –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ (–í JUPYTER)\n",
    "\n",
    "```python\n",
    "from ollama import chat\n",
    "\n",
    "response = chat(\n",
    "    model=\"mistral_text_ft\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"–ß—Ç–æ —Ç–∞–∫–æ–µ RAG?\"}]\n",
    ")\n",
    "\n",
    "print(response[\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ API (FASTAPI)\n",
    "\n",
    "---\n",
    "\n",
    "–§–∞–π–ª:\n",
    "\n",
    "```\n",
    "ml_championship/api.py\n",
    "```\n",
    "\n",
    "\n",
    "## –ö–æ–¥ API\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from ollama import chat\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Query(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "def ask(q: Query):\n",
    "    response = chat(\n",
    "        model=\"mistral:7b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": q.text}]\n",
    "    )\n",
    "    return {\"answer\": response[\"message\"][\"content\"]}\n",
    "```\n",
    "\n",
    "\n",
    "## –ó–∞–ø—É—Å–∫ API (–í –¢–ï–†–ú–ò–ù–ê–õ–ï)\n",
    "\n",
    "```bash\n",
    "cd ml_championship\n",
    "source venv/bin/activate\n",
    "uvicorn api:app --reload\n",
    "```\n",
    "\n",
    "–û—Ç–∫—Ä—ã—Ç—å –≤ –±—Ä–∞—É–∑–µ—Ä–µ:\n",
    "\n",
    "```\n",
    "http://127.0.0.1:8000/docs\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ TELEGRAM-–ë–û–¢\n",
    "\n",
    "---\n",
    "\n",
    "##  –°–æ–∑–¥–∞–Ω–∏–µ –±–æ—Ç–∞\n",
    "\n",
    "1. Telegram ‚Üí `@BotFather`\n",
    "2. `/start`\n",
    "3. `/newbot`\n",
    "4. —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å TOKEN\n",
    "\n",
    "\n",
    "## –ö–æ–¥ Telegram-–±–æ—Ç–∞\n",
    "\n",
    "```python\n",
    "from telegram.ext import Application, MessageHandler, filters\n",
    "from ollama import chat\n",
    "\n",
    "TOKEN = \"PASTE_YOUR_TOKEN_HERE\"\n",
    "\n",
    "async def handle_message(update, context):\n",
    "    user_text = update.message.text\n",
    "\n",
    "    response = chat(\n",
    "        model=\"mistral:7b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": user_text}]\n",
    "    )\n",
    "\n",
    "    await update.message.reply_text(response[\"message\"][\"content\"])\n",
    "\n",
    "app = Application.builder().token(TOKEN).build()\n",
    "app.add_handler(MessageHandler(filters.TEXT, handle_message))\n",
    "app.run_polling()\n",
    "```\n",
    "\n",
    "\n",
    "## –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ (–í –¢–ï–†–ú–ò–ù–ê–õ–ï)\n",
    "\n",
    "```bash\n",
    "cd ml_championship/bot\n",
    "source ../venv/bin/activate\n",
    "python bot.py\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d2562",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2703396e",
   "metadata": {},
   "source": [
    "# üîπ –î–û–û–ë–£–ß–ï–ù–ò–ï RAG –≤–∞—Ä–∏–∞–Ω—Ç –¥–≤–µ –º–æ–¥–µ–ª–∏ (Embedding + LLM) –æ—Å–æ–±–µ–Ω–Ω–æ –ø–æ–ª–µ–∑–Ω–æ –µ—Å–ª–∏ –¥–∞–Ω –æ–≥—Ä–æ–º–Ω—ã–π —Ç–µ–∫—Å—Ç –Ω–µ —Ñ–æ—Ä–º–∞—Ç–∞ –≤–æ–ø—Ä–æ—Å - –æ—Ç–≤–µ—Ç\n",
    "\n",
    "--- \n",
    "\n",
    "1. –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    " - –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è embedding‚Äë–º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä nomic-embed-text)\n",
    "–ó–∞–¥–∞—á–∞: –ø—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∫—É—Å–∫–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ —Å–º—ã—Å–ª—É\n",
    " - –ù–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç, —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è\n",
    "\n",
    "2. –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞\n",
    " - –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∞—è LLM (–Ω–∞–ø—Ä–∏–º–µ—Ä mistral:7b)\n",
    "–ó–∞–¥–∞—á–∞: –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫—Ä–∞—Å–∏–≤–æ –∏ —Å–≤—è–∑–Ω–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å\n",
    "\n",
    "üîπ –ü–æ—à–∞–≥–æ–≤–∞—è —Å—Ö–µ–º–∞\n",
    "```bash\n",
    "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –∑–∞–¥–∞—ë—Ç –≤–æ–ø—Ä–æ—Å\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "Embedding-–º–æ–¥–µ–ª—å ‚Üí –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –≤—Å–µ —á–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞ –≤ –≤–µ–∫—Ç–æ—Ä—ã\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ ‚Üí –≤—ã–±–∏—Ä–∞–µ–º —Ç–æ–ø-k —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "LLM (–Ω–∞–ø—Ä–∏–º–µ—Ä Mistral) ‚Üí —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –æ—Ç–≤–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —ç—Ç–∏—Ö —á–∞–Ω–∫–æ–≤\n",
    "        ‚îÇ\n",
    "        ‚ñº\n",
    "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç\n",
    "```\n",
    "## –£—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
    "\n",
    "``` bash\n",
    "ollama pull nomic-embed-text\n",
    "# –µ—Å–ª–∏ –±–æ–ª–µ–µ —Ç–æ—á–Ω—É—é:\n",
    "# ollama pull mxbai-embed-large\n",
    "```\n",
    "\n",
    "\n",
    "``` bash\n",
    "ollama pull mistral:7b\n",
    "```\n",
    "\n",
    "## –ü—Ä–∏–º–µ—Ä –∫–æ–¥–∞ (–Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ text.txt)\n",
    "\n",
    "```python\n",
    "from ollama import embeddings, chat\n",
    "import numpy as np\n",
    "\n",
    "# -----------------------------\n",
    "# 1) —á–∏—Ç–∞–µ–º —Ç–µ–∫—Å—Ç –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏\n",
    "# -----------------------------\n",
    "with open(\"../data/document.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "def split_text(text, chunk_size=400):\n",
    "    words = text.split()\n",
    "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "chunks = split_text(text)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) —Å–æ–∑–¥–∞—ë–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —á–∞–Ω–∫–æ–≤\n",
    "# -----------------------------\n",
    "chunk_embeddings = []\n",
    "for chunk in chunks:\n",
    "    emb = embeddings(\n",
    "        model=\"nomic-embed-text\",  # embedding-–º–æ–¥–µ–ª—å\n",
    "        prompt=chunk\n",
    "    )\n",
    "    chunk_embeddings.append(emb[\"embedding\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 3) —Ñ—É–Ω–∫—Ü–∏—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞\n",
    "# -----------------------------\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def embeddings_fn(text):\n",
    "    emb = embeddings(model=\"nomic-embed-text\", prompt=text)\n",
    "    return emb[\"embedding\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 4) –ø–æ–∏—Å–∫ —Ç–æ–ø-k —á–∞–Ω–∫–æ–≤\n",
    "# -----------------------------\n",
    "def retrieve_chunks(question, chunks, embeddings, top_k=3):\n",
    "    q_emb = embeddings_fn(question)\n",
    "    similarities = [cosine_similarity(q_emb, e) for e in embeddings]\n",
    "    top_indices = np.argsort(similarities)[-top_k:]\n",
    "    return [chunks[i] for i in top_indices]\n",
    "\n",
    "# -----------------------------\n",
    "# 5) —Å–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º prompt –¥–ª—è LLM\n",
    "# -----------------------------\n",
    "def build_prompt(context_chunks, question):\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    prompt = f\"\"\"\n",
    "–¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å,\n",
    "–∏—Å–ø–æ–ª—å–∑—É—è –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n",
    "\n",
    "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n",
    "{context}\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å:\n",
    "{question}\n",
    "\n",
    "–û—Ç–≤–µ—Ç:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "# -----------------------------\n",
    "# 6) –∏—Ç–æ–≥–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è RAG —Å –¥–≤—É–º—è –º–æ–¥–µ–ª—è–º–∏\n",
    "# -----------------------------\n",
    "def rag_answer(question):\n",
    "    relevant_chunks = retrieve_chunks(question, chunks, chunk_embeddings)\n",
    "    prompt = build_prompt(relevant_chunks, question)\n",
    "    response = chat(\n",
    "        model=\"mistral:7b\",  # LLM –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 7) —Ç–µ—Å—Ç\n",
    "# -----------------------------\n",
    "print(rag_answer(\"–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö?\"))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef96cf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
