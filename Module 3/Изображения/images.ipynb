{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea737997",
   "metadata": {},
   "source": [
    " # üîπ 1) –Ω–∞—á–∞–ª—å–Ω–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077ee6de",
   "metadata": {},
   "source": [
    "1. –∏–º–ø–æ—Ä—Ç—ã –±–∏–±–ª–∏–æ—Ç–µ–∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38f2fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d30737",
   "metadata": {},
   "source": [
    "2. –ø—Ä–∏–≤–µ–¥–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∫ –æ–¥–Ω–æ–º—É –≤–∏–¥—É –∏ —Ñ–æ—Ä–º–∞—Ç—É —Ç–µ–Ω–∑–æ—Ä–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac24b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "basic_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "from torchvision import datasets\n",
    "\n",
    "train_ds = datasets.ImageFolder(\"data_fruits/train\", transform=basic_tfms)\n",
    "val_ds   = datasets.ImageFolder(\"data_fruits/val\",   transform=basic_tfms)\n",
    "test_ds  = datasets.ImageFolder(\"data_fruits/test\",  transform=basic_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246ca42",
   "metadata": {},
   "source": [
    "3. —Å–æ—â–¥–∞–µ–º ImageFolder, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–µ–Ω, —á—Ç–æ–±—ã –Ω–µ –ø–∏—Å–∞—Ç—å —Å–≤–æ–π –∫–æ–¥ –æ–±—Ö–æ–¥–∞ –ø–∞–ø–æ–∫\n",
    "ImageFolder:\n",
    "- —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ –ø–æ–¥–ø–∞–ø–∫–∏ –≤ train/val/test,\n",
    "- —Å—á–∏—Ç–∞–µ—Ç –∫–∞–∂–¥—É—é –ø–æ–¥–ø–∞–ø–∫—É –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫–ª–∞—Å—Å–æ–º,\n",
    "- –∫–∞–∂–¥–æ–º—É –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é –¥–∞—ë—Ç –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4e1891",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "train_ds = datasets.ImageFolder(root=\"data_fruits/train\", transform=basic_tfms)\n",
    "val_ds   = datasets.ImageFolder(root=\"data_fruits/val\",   transform=basic_tfms)\n",
    "test_ds  = datasets.ImageFolder(root=\"data_fruits/test\",  transform=basic_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d3d3e5",
   "metadata": {},
   "source": [
    "4. DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "138e1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "test_dl  = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b463ebe",
   "metadata": {},
   "source": [
    "# üîπ 2) –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π ML –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\n",
    "\n",
    "1 –≤–∞—Ä–∏–∞–Ω—Ç \n",
    "1. –ü—Ä–µ–≤—Ä–∞—Ç–∏—Ç—å –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ —á–∏—Å–ª–æ–≤–æ–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "—Ç.–µ\n",
    "    - –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ RGB;\n",
    "    - —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏);\n",
    "    - —Å—á–∏—Ç–∞–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É (R, G, B);\n",
    "    - —Å–æ–µ–¥–∏–Ω—è–µ–º —Ç—Ä–∏ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä.\n",
    "2. –û–±—É—á–∏—Ç—å –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, SVM) –Ω–∞ —ç—Ç–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö.\n",
    "3. –ü–æ–ª—É—á–∏—Ç—å –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ (accuracy, precision, recall, F1).\n",
    "\n",
    "2 –≤–∞—Ä–∏–∞–Ω—Ç\n",
    "–ø—Ä–æ—Å—Ç–æ –±–æ–ª—å—à–µ —Ä–∞–∑–º–µ—Ä + flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0e206",
   "metadata": {},
   "source": [
    "1 –≤–∞—Ä–∏–∞–Ω—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2f0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted([p.name for p in Path(\"data_fruits/train\").iterdir() if p.is_dir()])\n",
    "\n",
    "def img_features1(path, bins=16):\n",
    "    \"\"\"\n",
    "    –ò–∑ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–∏ –¥–µ–ª–∞–µ–º –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "    - –ø–µ—Ä–µ–≤–æ–¥–∏–º –∫–∞—Ä—Ç–∏–Ω–∫—É –≤ RGB;\n",
    "    - —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏);\n",
    "    - —Å—á–∏—Ç–∞–µ–º –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É (R, G, B);\n",
    "    - —Å–æ–µ–¥–∏–Ω—è–µ–º —Ç—Ä–∏ –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä.\n",
    "    - –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É (–µ—â—ë 6 —á–∏—Å–µ–ª)\n",
    "    \"\"\"\n",
    "    # 1) RGB\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    # 2) —É–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä\n",
    "    img = img.resize((128, 128))\n",
    "\n",
    "    # 3) –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º\n",
    "    arr = np.array(img)\n",
    "    hist_list = []\n",
    "    for ch in range(3): \n",
    "        channel = arr[..., ch]\n",
    "        hist, _ = np.histogram(\n",
    "            channel,\n",
    "            bins=bins,\n",
    "            range=(0, 256),\n",
    "            density=True\n",
    "        )\n",
    "        hist_list.append(hist)\n",
    "\n",
    "\n",
    "    means = arr.mean(axis=(0, 1))\n",
    "    stds  = arr.std(axis=(0, 1))\n",
    "\n",
    "    # 4) –æ–¥–∏–Ω –æ–±—â–∏–π –≤–µ–∫—Ç–æ—Ä\n",
    "    features = np.concatenate(hist_list + [means, stds])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be427229",
   "metadata": {},
   "source": [
    "2 –≤–∞—Ä–∏–∞–Ω—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274c9f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = sorted([p.name for p in Path(\"data_fruits/train\").iterdir() if p.is_dir()])\n",
    "train_dir = Path(\"data_fruits/train\")\n",
    "\n",
    "def img_features2(path):\n",
    "    \"\"\"\n",
    "    –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –∫–∞—Ä—Ç–∏–Ω–∫–∏:\n",
    "    - RGB\n",
    "    - —É–º–µ–Ω—å—à–∞–µ–º –¥–æ 32x32\n",
    "    - —Ä–∞—Å–ø–ª—é—â–∏–≤–∞–µ–º –≤ –æ–¥–∏–Ω –≤–µ–∫—Ç–æ—Ä (32*32*3 = 3072 —á–∏—Å–ª–∞)\n",
    "    \"\"\"\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    img = img.resize((32, 32))\n",
    "    arr = np.array(img) / 255.0\n",
    "    return arr.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeec4f2",
   "metadata": {},
   "source": [
    "–¥–µ–ª–∞–µ–º –º–∞—Ç—Ä–∏—Ü—ã x –∏ y –¥–ª—è 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf2b447",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for cls_idx, cls in enumerate(class_names):\n",
    "    folder = train_dir / cls\n",
    "    for name in os.listdir(folder):\n",
    "        if name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            X.append(img_features2(folder / name))\n",
    "            y.append(cls_idx)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a1dd70",
   "metadata": {},
   "source": [
    "–¥–µ–ª–∞–µ–º –º–∞—Ç—Ä–∏—Ü—ã x –∏ y –¥–ª—è 1 –≤–∞—Ä–∏–∞–Ω—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29bcee9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = [], []\n",
    "\n",
    "for cls_idx, cls in enumerate(class_names):\n",
    "    folder = Path(\"data_fruits/train\") / cls\n",
    "    for name in os.listdir(folder):\n",
    "        if name.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            path = folder / name\n",
    "            X.append(img_features1(path))  # –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "            y.append(cls_idx)  # –Ω–æ–º–µ—Ä –∫–ª–∞—Å—Å–∞\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55f9e4b",
   "metadata": {},
   "source": [
    "–¥–µ–ª–∏–º –Ω–∞ train –∏ val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23db1a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7b46ed",
   "metadata": {},
   "source": [
    "–æ–±—É—á–∏–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccb6ffc",
   "metadata": {},
   "source": [
    "1. –ª–æ–≥–∏—Å—Ç–∏—á—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3349f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     apple fruit       0.25      0.20      0.22         5\n",
      "    banana fruit       0.17      0.20      0.18         5\n",
      "    cherry fruit       0.50      0.33      0.40         6\n",
      "   chickoo fruit       0.43      0.50      0.46         6\n",
      "    grapes fruit       0.40      0.33      0.36         6\n",
      "      kiwi fruit       0.50      0.80      0.62         5\n",
      "     mango fruit       0.25      0.20      0.22         5\n",
      "    orange fruit       1.00      0.60      0.75         5\n",
      "strawberry fruit       0.43      0.60      0.50         5\n",
      "\n",
      "        accuracy                           0.42        48\n",
      "       macro avg       0.44      0.42      0.41        48\n",
      "    weighted avg       0.44      0.42      0.41        48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Heckfy\\atom\\REA\\Preparing_Rea2026\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred_lr = log_reg.predict(X_val)\n",
    "print(classification_report(y_val, y_pred_lr, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d802ac64",
   "metadata": {},
   "source": [
    "—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –æ—á–µ–Ω—å —Ç–∫ –¥–∞–Ω–Ω—ã—Ö –æ—á–µ–Ω—å –º–∞–ª–æ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d977902",
   "metadata": {},
   "source": [
    "2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb3f6daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     apple fruit       0.30      0.60      0.40         5\n",
      "    banana fruit       0.29      0.40      0.33         5\n",
      "    cherry fruit       0.00      0.00      0.00         6\n",
      "   chickoo fruit       0.67      0.33      0.44         6\n",
      "    grapes fruit       0.50      0.50      0.50         6\n",
      "      kiwi fruit       0.40      0.80      0.53         5\n",
      "     mango fruit       0.00      0.00      0.00         5\n",
      "    orange fruit       0.80      0.80      0.80         5\n",
      "strawberry fruit       0.33      0.40      0.36         5\n",
      "\n",
      "        accuracy                           0.42        48\n",
      "       macro avg       0.37      0.43      0.37        48\n",
      "    weighted avg       0.37      0.42      0.37        48\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Heckfy\\atom\\REA\\Preparing_Rea2026\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "E:\\Heckfy\\atom\\REA\\Preparing_Rea2026\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "E:\\Heckfy\\atom\\REA\\Preparing_Rea2026\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_clf = SVC(kernel=\"rbf\", C=5, gamma=\"scale\")\n",
    "svm_clf.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred_svm = svm_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred_svm, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904edad7",
   "metadata": {},
   "source": [
    "3. random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a64ab805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     apple fruit       0.33      0.20      0.25         5\n",
      "    banana fruit       0.25      0.40      0.31         5\n",
      "    cherry fruit       0.33      0.17      0.22         6\n",
      "   chickoo fruit       0.75      0.50      0.60         6\n",
      "    grapes fruit       0.50      0.67      0.57         6\n",
      "      kiwi fruit       0.71      1.00      0.83         5\n",
      "     mango fruit       0.67      0.40      0.50         5\n",
      "    orange fruit       1.00      0.80      0.89         5\n",
      "strawberry fruit       0.38      0.60      0.46         5\n",
      "\n",
      "        accuracy                           0.52        48\n",
      "       macro avg       0.55      0.53      0.52        48\n",
      "    weighted avg       0.55      0.52      0.51        48\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_clf.fit(X_tr, y_tr)\n",
    "\n",
    "y_pred_rf = rf_clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred_rf, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebc91df",
   "metadata": {},
   "source": [
    "üîπ 3) –ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ \n",
    "\n",
    "–Ω–∞ –ø—Ä–∏–º–µ—Ä–µ ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381d4f6",
   "metadata": {},
   "source": [
    "- –≤—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (`device`);\n",
    "- —Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (`num_classes`);\n",
    "- –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33fbdba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# device: –∫—É–¥–∞ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –∏ –¥–∞–Ω–Ω—ã–µ (cuda, –µ—Å–ª–∏ –µ—Å—Ç—å GPU, –∏–Ω–∞—á–µ cpu)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# num_classes: —Å–∫–æ–ª—å–∫–æ —É –Ω–∞—Å —Ä–∞–∑–Ω—ã—Ö —Ñ—Ä—É–∫—Ç–æ–≤ (–∫–ª–∞—Å—Å–æ–≤)\n",
    "num_classes = len(class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f6dba",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞—ë–º ResNet18 (–º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–∞–∫–∂–µ ResNet50 / ResNet101 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5cd62bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# —Å–æ–∑–¥–∞—ë–º ResNet18 –±–µ–∑ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤\n",
    "resnet = models.resnet18(weights=None)\n",
    "\n",
    "#resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) - –æ–±—É—á–µ–Ω–∏–µ —Å –≤–µ—Å–∞–º–∏(–º–æ–π –Ω–æ—É—Ç –ø—Ä–æ—Å—Ç–æ –Ω–µ —Ç—è–Ω–µ—Ç)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60e0fb",
   "metadata": {},
   "source": [
    "–ú–µ–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –ø–æ–¥ –Ω–∞—à–∏ –∫–ª–∞—Å—Å—ã\n",
    "\n",
    "- `resnet.fc` ‚Äî –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π.\n",
    "- –£ –Ω–µ–≥–æ –µ—Å—Ç—å –≤—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å `in_features`.\n",
    "- –ó–∞–º–µ–Ω—è–µ–º –µ–≥–æ –Ω–∞ `nn.Linear(in_features, num_classes)`, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –º–æ–≥–ª–∞ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞—à–∏ –∫–ª–∞—Å—Å—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60f02605",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed10ae7",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "- –ü–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –Ω–∞ `device`.\n",
    "- –ó–∞–¥–∞—ë–º:\n",
    "  - `criterion` ‚Äî —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å (CrossEntropyLoss –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∑–∞–¥–∞—á–∏).\n",
    "  - `optimizer_resnet` ‚Äî –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä Adam —Å –º–∞–ª–µ–Ω—å–∫–∏–º —à–∞–≥–æ–º –æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47a6dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ø–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –Ω–∞ GPU/CPU\n",
    "resnet = resnet.to(device)\n",
    "\n",
    "# —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –ª–æ–≥–∏—Ç—ã –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: Adam, –æ–±—É—á–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "optimizer_resnet = optim.Adam(\n",
    "    resnet.parameters(),\n",
    "    lr=1e-4,\n",
    "    weight_decay=1e-4,   # —ç—Ç–æ L2‚Äë—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è, –æ–Ω–∞ —à—Ç—Ä–∞—Ñ—É–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ –≤–µ—Å–∞ –∏ –ø–æ–º–æ–≥–∞–µ—Ç –±–æ—Ä–æ—Ç—å—Å—è —Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ–º\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e0581",
   "metadata": {},
   "source": [
    "–æ–±—É—á–∞–µ–º —ç–ø–æ—Ö—É\n",
    "–ù–∞ –≤—Ö–æ–¥:\n",
    "- `model` ‚Äî –Ω–∞—à–∞ ResNet18;\n",
    "- `loader` ‚Äî train_dl (–±–∞—Ç—á–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –∏ –º–µ—Ç–æ–∫);\n",
    "- `optimizer` ‚Äî –æ–±—ä–µ–∫—Ç Adam.\n",
    "\n",
    "–ü—Ä–æ—Ü–µ—Å—Å:\n",
    "1. –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è `model.train()`.\n",
    "2. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞:\n",
    "   - –ø–µ—Ä–µ–Ω–æ—Å–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ `device`;\n",
    "   - –æ–±–Ω—É–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã;\n",
    "   - —Å—á–∏—Ç–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (`logits`);\n",
    "   - —Å—á–∏—Ç–∞–µ–º loss;\n",
    "   - —Å—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã (`backward`);\n",
    "   - –¥–µ–ª–∞–µ–º —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (`step`);\n",
    "   - –∫–æ–ø–∏–º loss –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "3. –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π loss –∏ accuracy –ø–æ –≤—Å–µ–π —ç–ø–æ—Ö–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8710752",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer):\n",
    "    model.train()     \n",
    "\n",
    "    total_loss = 0.0             \n",
    "    total_correct = 0        \n",
    "    total = 0                        \n",
    "\n",
    "    for images, labels in loader:\n",
    "        # –ø–µ—Ä–µ–Ω–æ—Å–∏–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –∏ –º–µ—Ç–∫–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # —à–∞–≥ 1: –æ–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # —à–∞–≥ 2: –ø—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (forward)\n",
    "        logits = model(images)\n",
    "\n",
    "        # —à–∞–≥ 3: —Å—á–∏—Ç–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        # —à–∞–≥ 4: –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏\n",
    "        loss.backward()\n",
    "\n",
    "        # —à–∞–≥ 5: –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞\n",
    "        optimizer.step()\n",
    "\n",
    "        # —Å—á–∏—Ç–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –±–∞—Ç—á—É\n",
    "        batch_size = images.size(0)\n",
    "        total_loss += loss.item() * batch_size\n",
    "\n",
    "        # –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å = –∏–Ω–¥–µ–∫—Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–≥–æ –ª–æ–≥–∏—Ç–∞\n",
    "        preds = logits.argmax(1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total += batch_size\n",
    "\n",
    "    # —Å—Ä–µ–¥–Ω–∏–π loss –∏ accuracy –ø–æ —ç–ø–æ—Ö–µ\n",
    "    avg_loss = total_loss / total\n",
    "    avg_acc = total_correct / total\n",
    "    return avg_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec623c7",
   "metadata": {},
   "source": [
    "–æ—Ü–µ–Ω–∫–∞\n",
    "–ù–∞ –≤—Ö–æ–¥:\n",
    "- `model` ‚Äî ResNet18;\n",
    "- `loader` ‚Äî val_dl –∏–ª–∏ test_dl.\n",
    "\n",
    "–ü—Ä–æ—Ü–µ—Å—Å:\n",
    "1. –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ `model.eval()`.\n",
    "2. –û—Ç–∫–ª—é—á–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã `torch.no_grad()`.\n",
    "3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞:\n",
    "   - —Å—á–∏—Ç–∞–µ–º loss –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è;\n",
    "   - –∫–æ–ø–∏–º loss –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–µ—Ä–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤;\n",
    "   - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –º–µ—Ç–∫–∏ –∏ –≤—Å–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.\n",
    "4. –í–æ–∑–≤—Ä–∞—â–∞–µ–º:\n",
    "   - —Å—Ä–µ–¥–Ω–∏–π loss,\n",
    "   - accuracy,\n",
    "   - –º–∞—Å—Å–∏–≤ –≤—Å–µ—Ö –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ `y_true`,\n",
    "   - –º–∞—Å—Å–∏–≤ –≤—Å–µ—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π `y_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc8f1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    model.eval()      \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for images, labels in loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            batch_size = images.size(0)\n",
    "            total_loss += loss.item() * batch_size\n",
    "\n",
    "            preds = logits.argmax(1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–ª—è –æ—Ç—á—ë—Ç–æ–≤\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / total\n",
    "    avg_acc = total_correct / total\n",
    "    y_true = np.concatenate(all_labels)\n",
    "    y_pred = np.concatenate(all_preds)\n",
    "    return avg_loss, avg_acc, y_true, y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a0d9cd",
   "metadata": {},
   "source": [
    "–û–±—É—á–µ–Ω–∏–µ ResNet18 –ø–æ —ç–ø–æ—Ö–∞–º\n",
    "\n",
    "- `num_epochs` ‚Äî —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ train_dl.\n",
    "- –ù–∞ –∫–∞–∂–¥–æ–π —ç–ø–æ—Ö–µ:\n",
    "  - —Å—á–∏—Ç–∞–µ–º `train_loss`, `train_acc` –Ω–∞ train_dl;\n",
    "  - —Å—á–∏—Ç–∞–µ–º `val_loss`, `val_acc` –Ω–∞ val_dl;\n",
    "  - –ø–µ—á–∞—Ç–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d4e7008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResNet] Epoch 1/5 | train_loss=1.9800, train_acc=0.297 | val_loss=2.2722, val_acc=0.111\n",
      "[ResNet] Epoch 2/5 | train_loss=1.2593, train_acc=0.590 | val_loss=2.9676, val_acc=0.111\n",
      "[ResNet] Epoch 3/5 | train_loss=0.9328, train_acc=0.686 | val_loss=3.8259, val_acc=0.111\n",
      "[ResNet] Epoch 4/5 | train_loss=0.7504, train_acc=0.782 | val_loss=4.1440, val_acc=0.130\n",
      "[ResNet] Epoch 5/5 | train_loss=0.5649, train_acc=0.833 | val_loss=4.0314, val_acc=0.148\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5  # –º–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å, –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –ª—É—á—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_one_epoch(resnet, train_dl, optimizer_resnet)\n",
    "    val_loss, val_acc, _, _ = evaluate(resnet, val_dl)\n",
    "\n",
    "    print(\n",
    "        f\"[ResNet] Epoch {epoch+1}/{num_epochs} | \"\n",
    "        f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n",
    "        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab57f16a",
   "metadata": {},
   "source": [
    "–§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ\n",
    "\n",
    "- —Å—á–∏—Ç–∞–µ–º `test_loss` –∏ `test_acc` –Ω–∞ `test_dl`;\n",
    "- —Å—Ç—Ä–æ–∏–º `classification_report` (precision, recall, F1 –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É);\n",
    "- –ø–æ–ª—É—á–∞–µ–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫ `confusion_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30a7aa2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet18 test_loss: 4.439445676474736\n",
      "ResNet18 test_acc : 0.20689655172413793\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     apple fruit       0.00      0.00      0.00         7\n",
      "    banana fruit       0.83      0.83      0.83         6\n",
      "    cherry fruit       0.00      0.00      0.00         7\n",
      "   chickoo fruit       0.12      1.00      0.22         6\n",
      "    grapes fruit       0.00      0.00      0.00         7\n",
      "      kiwi fruit       1.00      0.17      0.29         6\n",
      "     mango fruit       0.00      0.00      0.00         6\n",
      "    orange fruit       0.00      0.00      0.00         6\n",
      "strawberry fruit       0.00      0.00      0.00         7\n",
      "\n",
      "        accuracy                           0.21        58\n",
      "       macro avg       0.22      0.22      0.15        58\n",
      "    weighted avg       0.20      0.21      0.14        58\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 6, 0, 0, 1, 0, 0],\n",
       "       [0, 5, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 6, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 5, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 6, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 4, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 7, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_acc, y_true_res, y_pred_res = evaluate(resnet, test_dl)\n",
    "\n",
    "print(\"ResNet18 test_loss:\", test_loss)\n",
    "print(\"ResNet18 test_acc :\", test_acc)\n",
    "\n",
    "print(classification_report(\n",
    "    y_true_res,\n",
    "    y_pred_res,\n",
    "    target_names=class_names,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "cm_resnet = confusion_matrix(y_true_res, y_pred_res)\n",
    "cm_resnet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc57ec9",
   "metadata": {},
   "source": [
    "–ø–ª–æ—Ö–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–∫ –Ω–µ—Ç –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874064f",
   "metadata": {},
   "source": [
    "# –¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å  EfficientNet-B0 –∏–ª–∏ Vision Transformer ViT-B/16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccc1e21",
   "metadata": {},
   "source": [
    "# üîπ –ú–µ—Ç—Ä–∏–∫–∏ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "229f054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.20689655172413793\n",
      "Macro precision: 0.21730914588057446\n",
      "Macro recall   : 0.2222222222222222\n",
      "Macro F1       : 0.1485810485810486\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     apple fruit       0.00      0.00      0.00         7\n",
      "    banana fruit       0.83      0.83      0.83         6\n",
      "    cherry fruit       0.00      0.00      0.00         7\n",
      "   chickoo fruit       0.12      1.00      0.22         6\n",
      "    grapes fruit       0.00      0.00      0.00         7\n",
      "      kiwi fruit       1.00      0.17      0.29         6\n",
      "     mango fruit       0.00      0.00      0.00         6\n",
      "    orange fruit       0.00      0.00      0.00         6\n",
      "strawberry fruit       0.00      0.00      0.00         7\n",
      "\n",
      "        accuracy                           0.21        58\n",
      "       macro avg       0.22      0.22      0.15        58\n",
      "    weighted avg       0.20      0.21      0.14        58\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "# accuracy \n",
    "acc = accuracy_score(y_true_res, y_pred_res)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "# macro precision/recall/F1\n",
    "prec = precision_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "rec  = recall_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "f1   = f1_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(\"Macro precision:\", prec)\n",
    "print(\"Macro recall   :\", rec)\n",
    "print(\"Macro F1       :\", f1)\n",
    "\n",
    "# –ø–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á—ë—Ç –ø–æ –∫–∞–∂–¥–æ–º—É –∫–ª–∞—Å—Å—É\n",
    "print(classification_report(y_true_res, y_pred_res, target_names=class_names, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672fb27c",
   "metadata": {},
   "source": [
    "- accuracy_score –¥–∞—ë—Ç –æ–±—â—É—é –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n",
    "\n",
    "- precision_score/recall_score/f1_score —Å average=\"macro\" –¥–∞—é—Ç —Å—Ä–µ–¥–Ω–µ–µ –ø–æ –∫–ª–∞—Å—Å–∞–º.\n",
    "\n",
    "- –î–ª—è **accuracy, precision, recall, F1**: –≤—Å–µ–≥–¥–∞ **—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cac471-b36a-42c1-9e5c-392586c48da9",
   "metadata": {},
   "source": [
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ñ—Ä—É–∫—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "386cfeff-ace6-41a2-b56a-fad451b5e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet.state_dict(), 'fruit_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf6eb5-de85-459c-9174-280fa5bc9174",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca344d-991e-47f7-b90a-463f41534798",
   "metadata": {},
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "–ß–∞—Å—Ç–æ –Ω–∞ —á–µ–º–ø–∏–æ–Ω–∞—Ç–∞—Ö –ø—Ä–æ—Å—è—Ç –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –ø–æ–º–æ—â—å—é AirFlow. \n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –∫–ª–∞—Å—Å–æ–≤, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –±—É–¥–µ—Ç –ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤ Airflow –∏–ª–∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤ –±—É–¥—É—â–µ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "029d9f88-9423-4812-bbf6-0265ec56a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tuning_fruit(new_data: DataLoader) -> None:\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å, –ø–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ gpu –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 9)\n",
    "    model.load_state_dict(torch.load(\"fruit_model.pth\"))\n",
    "    model.to(device)\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –º–æ–¥–µ–ª–∏\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # –ë–µ—Ä–µ–º —É–∂–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é\n",
    "    for epoch in range(5):\n",
    "        loss, acc = train_one_epoch(model, new_data, optimizer)\n",
    "        print(f\"{epoch}: loss={loss:.4f}, acc={acc:.4f}\")\n",
    "\n",
    "\n",
    "    # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "    test_ds  = datasets.ImageFolder(\"data_fruits/test\",  transform=basic_tfms)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "    \n",
    "    test_loss, test_acc, y_true_res, y_pred_res = evaluate(model, test_dl)\n",
    "\n",
    "    prec = precision_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "    rec  = recall_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "    f1   = f1_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "    acc = accuracy_score(y_true_res, y_pred_res)\n",
    "    metrics = {\n",
    "    \"accuracy\": acc,\n",
    "    \"macro_precision\": prec,\n",
    "    \"macro_recall\": rec,\n",
    "    \"macro_f1\": f1\n",
    "    }\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    torch.save(model.state_dict(), \"fruit_model.pth\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03157801-7acd-4af8-b646-b93738108bf6",
   "metadata": {},
   "source": [
    "## –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "047f6b29-267e-4ca7-aab0-54afbc8cb8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SimpleImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3207a4ea-32c5-49d9-8e2d-ba4f539fa2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_images = [\"coconut.jpg\", ]\n",
    "new_labels = [3, ]\n",
    "\n",
    "dataset = SimpleImageDataset(new_images, new_labels, transform=basic_tfms)\n",
    "fine_tuning_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3813251b-65be-4ff7-bce8-7d530a56c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=2.1684, acc=0.0000\n",
      "1: loss=1.4134, acc=1.0000\n",
      "2: loss=0.8753, acc=1.0000\n",
      "3: loss=0.5836, acc=1.0000\n",
      "4: loss=0.3925, acc=1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.2413793103448276,\n",
       " 'macro_precision': 0.25828460038986356,\n",
       " 'macro_recall': 0.25132275132275134,\n",
       " 'macro_f1': 0.17196969696969697}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tuning_fruit(fine_tuning_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482347c-dd6f-4ebd-a97a-802c0f97eb5c",
   "metadata": {},
   "source": [
    "#### –î–∞–Ω–Ω—ã–π —Å–ø–æ—Å–æ–± –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø—Ä–æ—Å—Ç–æ–≥–æ –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å —Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –Ω–µ –¥–æ–±–∞–≤–∏—Ç—å. –ï–≥–æ –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –ø—Ä–æ—Å—Ç—ã—Ö —Å–ª—É—á–∞—è—Ö –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "#### –ù–æ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å, —Ç–æ –Ω—É–∂–Ω–æ –¥–æ–±–∞–≤–ª—è—Ç—å –¥–∞–Ω–Ω—ã–µ –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É –Ω–∞–±–æ—Ä—É –∏ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å ResNet ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180d14d3-5323-4b5e-9288-faae01c0591e",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083d6ca-1b97-47e4-a4a7-051529c61ba4",
   "metadata": {},
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f83348-af89-41a5-8082-2d6d038375cb",
   "metadata": {},
   "source": [
    "–í–∞–∂–Ω–æ! –í –ø–∞–ø–∫–µ data_fruits –¥–æ–±–∞–≤–∏–ª–∏ –Ω–æ–≤—ã–π —Ç–∞—Ä–≥–µ—Ç coconut, –∫—É–¥–∞ –¥–æ–±–∞–≤–∏–ª–∏ –Ω–∞—à–µ –Ω–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b74ed16f-9275-4ac9-9477-b76b2416a740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple fruit\n",
      "banana fruit\n",
      "cherry fruit\n",
      "chickoo fruit\n",
      "coconut fruit\n",
      "grapes fruit\n",
      "kiwi fruit\n",
      "mango fruit\n",
      "orange fruit\n",
      "strawberry fruit\n"
     ]
    }
   ],
   "source": [
    "!ls data_fruits/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09cbb184-69c0-4235-a880-30129b1a863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrain_model(num_epochs=5):\n",
    "    basic_tfms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    train_ds = datasets.ImageFolder(\"data_fruits/train\", transform=basic_tfms)\n",
    "    val_ds   = datasets.ImageFolder(\"data_fruits/val\",   transform=basic_tfms)\n",
    "    test_ds  = datasets.ImageFolder(\"data_fruits/test\",  transform=basic_tfms)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "    test_dl  = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    class_names = sorted([p.name for p in Path(\"data_fruits/train\").iterdir() if p.is_dir()])\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    # –ø–µ—Ä–µ–Ω–æ—Å–∏–º –º–æ–¥–µ–ª—å –Ω–∞ GPU/CPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-4,\n",
    "        weight_decay=1e-4,\n",
    "    )\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_dl, optimizer)\n",
    "        val_loss, val_acc, _, _ = evaluate(model, val_dl)\n",
    "    \n",
    "        print(\n",
    "            f\"[ResNet] Epoch {epoch+1}/{num_epochs} | \"\n",
    "            f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n",
    "            f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\"\n",
    "        )\n",
    "\n",
    "    test_loss, test_acc, y_true_res, y_pred_res = evaluate(model, test_dl)\n",
    "    \n",
    "    prec = precision_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "    rec  = recall_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "    f1   = f1_score(y_true_res, y_pred_res, average=\"macro\", zero_division=0)\n",
    "    acc = accuracy_score(y_true_res, y_pred_res)\n",
    "    metrics = {\n",
    "    \"accuracy\": acc,\n",
    "    \"macro_precision\": prec,\n",
    "    \"macro_recall\": rec,\n",
    "    \"macro_f1\": f1\n",
    "    }\n",
    "\n",
    "    report = classification_report(\n",
    "    y_true_res,\n",
    "    y_pred_res,\n",
    "    target_names=class_names,\n",
    "    zero_division=0\n",
    "    )\n",
    "\n",
    "    torch.save(model.state_dict(), \"fruit_model.pth\")\n",
    "    \n",
    "    return metrics, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6d91417-848c-45a2-a266-0e9feacb134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResNet] Epoch 1/3 | train_loss=1.7258, train_acc=0.446 | val_loss=1.0011, val_acc=0.709\n",
      "[ResNet] Epoch 2/3 | train_loss=0.4085, train_acc=0.958 | val_loss=0.5445, val_acc=0.873\n",
      "[ResNet] Epoch 3/3 | train_loss=0.1650, train_acc=0.996 | val_loss=0.4003, val_acc=0.891\n",
      "{'accuracy': 0.8983050847457628, 'macro_precision': 0.8125, 'macro_recall': 0.8166666666666668, 'macro_f1': 0.8075757575757574}\n"
     ]
    }
   ],
   "source": [
    "metrics, report = retrain_model(num_epochs=3)\n",
    "\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "89865897-cb96-4af3-b46c-88ae37499318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "     apple fruit       0.88      1.00      0.93         7\n",
      "    banana fruit       1.00      1.00      1.00         6\n",
      "    cherry fruit       1.00      1.00      1.00         7\n",
      "   chickoo fruit       0.67      1.00      0.80         6\n",
      "   coconut fruit       0.00      0.00      0.00         1\n",
      "    grapes fruit       1.00      1.00      1.00         7\n",
      "      kiwi fruit       1.00      0.83      0.91         6\n",
      "     mango fruit       0.75      0.50      0.60         6\n",
      "    orange fruit       0.83      0.83      0.83         6\n",
      "strawberry fruit       1.00      1.00      1.00         7\n",
      "\n",
      "        accuracy                           0.90        59\n",
      "       macro avg       0.81      0.82      0.81        59\n",
      "    weighted avg       0.89      0.90      0.89        59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084e5854-2c41-4d07-84c5-2cba4c6bb8b8",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –≤–∏–¥–∏–º –∏–∑ –≤—ã–≤–æ–¥–∞, —É –Ω–∞—Å –¥–æ–±–∞–≤–∏–ª—Å—è –Ω–æ–≤—ã–π –∫–ª–∞—Å—Å coconut fruit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
